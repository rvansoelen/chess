{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.python.framework import ops\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "gHidden1OutputSize = 20\n",
    "pHidden1OutputSize = 50\n",
    "sHidden1OutputSize = 50\n",
    "hidden2OutputSize = 1\n",
    "seed = 0\n",
    "batchSize = 10\n",
    "trainingRatio = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the features into arrays\n",
    "inputFile = 'features.pckl'\n",
    "with open(inputFile, 'rb') as fid:\n",
    "    features = pickle.load(fid)\n",
    "random.shuffle(features)\n",
    "numSamples = len(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitAndBatchFeatures(batchSize, features):\n",
    "    random.shuffle(features)\n",
    "    globalFeatures = np.array([feature['globalFeatures'] for feature in features], dtype='float32')\n",
    "    pieceFeatures  = np.array([feature['pieceFeatures'] for feature in features], dtype='float32')\n",
    "    squareFeatures = np.array([feature['squareFeatures'] for feature in features], dtype='float32')\n",
    "    ranks = np.array([feature['moveRankings'] for feature in features], dtype='float32')\n",
    "    labels = np.array([feature['labels'] for feature in features], dtype='float32')\n",
    "\n",
    "    globalLength= globalFeatures.shape[1]\n",
    "    pieceLength = pieceFeatures.shape[1]\n",
    "    squareLength = squareFeatures.shape[1]\n",
    "    #reshape the arrays into batches\n",
    "    numSamples = globalFeatures.shape[0]\n",
    "    goalNumSamples = numSamples - numSamples%batchSize\n",
    "    #trim samples to fit evenly into batches\n",
    "    globalFeaturesTrim = globalFeatures[:goalNumSamples, :]\n",
    "    pieceFeaturesTrim = pieceFeatures[:goalNumSamples, :]\n",
    "    squareFeaturesTrim = squareFeatures[:goalNumSamples, :]\n",
    "    ranksTrim = ranks[:goalNumSamples, :]\n",
    "    labelsTrim = labels[:goalNumSamples, :]\n",
    "\n",
    "    #reshape into batches\n",
    "    globalBatchFeed = globalFeatruresTrim.reshape(-1, batchSize, globalLength)\n",
    "    piecelBatchFeed = pieceFeatruresTrim.reshape(-1, batchSize, pieceLength)\n",
    "    squareBatchFeed = squareFeatruresTrim.reshape(-1, batchSize, squarelLength)\n",
    "    rankBatchFeed = ranksTrim.reshape(-1, batchSize, 1)\n",
    "    labelBatchFeed = labelsTrim.reshape(-1, batchSize, 1)\n",
    "    \n",
    "    return globalBatchFeed, pieceBatchFeed, squareBatchFeed, rankBatchFeed, labelBatchFeed\n",
    "\n",
    "#Test\n",
    "splitAndBatchFeatures(batchSize, trainingFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "'''globalTensor = tf.convert_to_tensor(globalFeatures, dtype=tf.float32)\n",
    "pieceTensor = tf.convert_to_tensor(pieceFeatures, dtype=tf.float32)\n",
    "squareTensor = tf.convert_to_tensor(squareFeatures, dtype=tf.float32)\n",
    "rankTensor = tf.convert_to_tensor(ranks, dtype=tf.float32)\n",
    "labelTensor = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "\n",
    "train_input_queue = tf.train.slice_input_producer(\n",
    "                                    [globalFeatures, \n",
    "                                     pieceFeatures, \n",
    "                                     squareFeatures,\n",
    "                                     ranks,\n",
    "                                     labels],\n",
    "                                    shuffle=False)\n",
    "\n",
    "globalBatch, pieceBatch, squareBatch, rankBatch, labelBatch = tf.train.batch(\n",
    "                                    train_input_queue,\n",
    "                                    batch_size=batchSize\n",
    "                                    #,num_threads=1\n",
    "                                    )\n",
    "'''\n",
    "#global variables\n",
    "with tf.variable_scope('GlobalVariables'):\n",
    "    globalBatch = tf.placeholder(tf.float32, shape=(batchSize, globalLength), name='GlobalBatch')#1 bit (everything is really just floats)\n",
    "    #global hidden layer\n",
    "    gHidden1 = tf.layers.dense(globalBatch, gHidden1OutputSize, name='GlobalHiddenLayer1', activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.variance_scaling_initializer(seed=seed))\n",
    "    #testWeights = tf.get_variable(\"kernel\")\n",
    "#piece-centric variables\n",
    "with tf.variable_scope('Piece-CentricVariables'):\n",
    "    pieceBatch = tf.placeholder(tf.float32, shape=(batchSize, pieceLength), name='PieceBatch')\n",
    "\n",
    "    #piece-centric hidden layer\n",
    "    pHidden1 = tf.layers.dense(pieceBatch, pHidden1OutputSize, name='Piece-CentricHiddenLayer1', activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.variance_scaling_initializer(seed=seed))\n",
    "\n",
    "#square-centric variables\n",
    "with tf.variable_scope('Sqaure-CentricVariables'):\n",
    "    squareBatch = tf.placeholder(tf.float32, shape=(batchSize, squareLength), name='SquareBatch')\n",
    "    rankBatch = tf.placeholder(tf.float32, shape=(batchSize, 1), name='RankBatch')\n",
    "    squareRankBatch = tf.concat([squareBatch, rankBatch], 1)\n",
    "    #square-centric hidden layer\n",
    "    sHidden1 = tf.layers.dense(squareRankBatch, sHidden1OutputSize, name='Square-CentricHiddenLayer1', activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.variance_scaling_initializer(seed=seed))\n",
    "\n",
    "input2 = tf.concat([gHidden1, pHidden1, sHidden1], 1)\n",
    "logits = tf.layers.dense(input2, hidden2OutputSize, name='HiddenLayer2', activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.variance_scaling_initializer(seed=seed))\n",
    "probability = tf.nn.sigmoid(logits)\n",
    "labelBatch = tf.placeholder(tf.float32, shape=(batchSize, 1), name='LabelBatch')\n",
    "loss = tf.losses.sigmoid_cross_entropy(labelBatch, logits) #groundTruth must be of shape [batchSize, 1]\n",
    "optimizer = tf.train.AdadeltaOptimizer()\n",
    "trainer = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89, 19)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalFeatures.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchesPerEpoch = float(globalFeatures.shape[0])/batchSize\n",
    "numEpochs = 1\n",
    "saveDir = '/home/rvansoelen/rbmcData/savedModels/test/'\n",
    "saveFrequency = 1000\n",
    "saver = tf.train.Saver()\n",
    "lastSavePath = ''\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #print sess.run( weights = tf.get_variable(\"GlobalVariable/kernel\"))\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    lossRecord = []\n",
    "    for epoch in range(1):\n",
    "        globalBatchFeed, pieceBatchFeed, squareBatchFeed, rankBatchFeed, labelBatchFeed = splitAndBatchFeatures(batchSize, trainingFeatures)\n",
    "        for i in range(globalBatchFeed.shape[0]):\n",
    "            thisLoss = sess.run((loss, trainer), \n",
    "                feed_dict={globalBatch=globalBatchFeed[i], \n",
    "                          pieceBatch=pieceBatchFeed[i], \n",
    "                          squareBatch=squareBatchFeed[i], \n",
    "                          rankBatch=rankBatchFeed[i], \n",
    "                          labelBatch=labelBatchFeed[i]\n",
    "                          })[0]\n",
    "            lossRecord.append(thisLoss)\n",
    "            if i % saveFrequency == 0:\n",
    "                path = saveDir+'model_'+str(i)+'.ckpt'\n",
    "                lastSavePath = saver.save(sess, path)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    '''tf.saved_model.simple_save(sess,\n",
    "            saveDir,\n",
    "            inputs={'globalFeatures': train_input_queue[0],\n",
    "                    'pieceFeatures': train_input_queue[1], \n",
    "                    'squareFeatures': train_input_queue[2],\n",
    "                    'ranks' : train_input_queue[3], \n",
    "                    'labels' : train_input_queue[4]},\n",
    "            outputs={\"probability\": probability})'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3263891, 1.5419257]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/rvansoelen/rbmcData/savedModels/test/model_0.ckpt\n"
     ]
    }
   ],
   "source": [
    "#evaluate training data\n",
    "#Does something have to be disabled to stop learning??\n",
    "fp = 0\n",
    "fn = 0\n",
    "tp = 0\n",
    "tn = 0\n",
    "lossRecord = []\n",
    "predictionRecord = []\n",
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    i=0\n",
    "    #path = saveDir+'model_'+str(i)+'.ckpt'\n",
    "    saver.restore(sess, lastSavePath)\n",
    "    globalBatchFeed, pieceBatchFeed, squareBatchFeed, rankBatchFeed, labelBatchFeed = splitAndBatchFeatures(batchSize, trainingFeatures)\n",
    "    for i in range(globalBatchFeed.shape[0]):\n",
    "        thisProbability, thisLoss = sess.run((probability, loss), \n",
    "            feed_dict={globalBatch=globalBatchFeed[i], \n",
    "                      pieceBatch=pieceBatchFeed[i], \n",
    "                      squareBatch=squareBatchFeed[i], \n",
    "                      rankBatch=rankBatchFeed[i], \n",
    "                      labelBatch=labelBatchFeed[i]\n",
    "                      })\n",
    "        lossRecord.append(thisLoss)\n",
    "        predictionRecord.extend([(probability[index], label[index]) for range(batchSize)])\n",
    "\n",
    "accuracy = []\n",
    "recall = []\n",
    "precision = []\n",
    "f1 = []\n",
    "for threshold in linspace(start, stop, num=50):\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    for prob, label in predictionRecord:\n",
    "        fp += np.sum(np.logical_and(prob <  0.5, label >= 0.5))\n",
    "        fn += np.sum(np.logical_and(prob >= 0.5, label <  0.5))\n",
    "        tp += np.sum(np.logical_and(prob >= 0.5, label >= 0.5))\n",
    "        tn += np.sum(np.logical_and(prob <  0.5, label <  0.5))\n",
    "    accuracy.append(float(tp+tn)/(fp+fn+tp+tn))\n",
    "    recall.append(float(tp)/(tp+fn))\n",
    "    precision.append(float(tp)/tp+fp)\n",
    "    f1.append(2*recall[-1]*precision[-1]/(recall[-1]+precision[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(accuracy)\n",
    "plt.title('Accuracy')\n",
    "plt.show()\n",
    "plt.plot(f1)\n",
    "plt.title('F1 Score')\n",
    "plt.show()\n",
    "plt.plot(precision, recall)\n",
    "plt.title('ROC Curve')\n",
    "plt.ylabel('Recall')\n",
    "plt.xlabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
